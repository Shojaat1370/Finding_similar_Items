{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQshmAs0gcy_"
      },
      "source": [
        "# **Finding similar items**\n",
        "\n",
        "### Project for the **Algorithms for massive data course**\n",
        "\n",
        "\n",
        "MSc, Data Science for Economics*\n",
        "\n",
        "Shojaat Joodi Bigdilo\n",
        "\n",
        "June 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBv3mYjYhJDJ",
        "outputId": "fd5eabf1-cc13-45df-f0bd-7d94e675c6ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p7QCZgXgb8g"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkCUkBEMgb5e"
      },
      "outputs": [],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMbUdvWqgb3M"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCl_E-Fbgb0Z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'xxxxxxxxx'\n",
        "os.environ['KAGGLE_KEY'] = 'xxxxxxxxx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_fD3_YohvxM"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d asaniczka/1-3m-linkedin-jobs-and-skills-2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkpvFtXHhvui"
      },
      "outputs": [],
      "source": [
        "extract_to_path  = \"/content/gdrive/My Drive/Massive_Data_Project/Job_Dataset\"\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('1-3m-linkedin-jobs-and-skills-2024.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Uk6S2C1hvZs"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import isnan, when, count, col, countDistinct\n",
        "from pyspark.sql.functions import lower, regexp_replace, size\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
        "from pyspark.sql.types import ArrayType\n",
        "\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, MinHashLSH\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "\n",
        "import re\n",
        "import time\n",
        "import string\n",
        "import datetime\n",
        "import warnings\n",
        "import numpy as np\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3kWgxGxjJfH"
      },
      "outputs": [],
      "source": [
        "conf = SparkConf().setAppName(\"Similar_Documents\")\n",
        "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "type(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46Rt4kzBWhet"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/gdrive/My Drive/Massive_Data_Project/Job_Dataset/job_summary.csv\"\n",
        "\n",
        "df_Dataset = spark.read.csv(file_path, header=True, inferSchema=True, multiLine=True, escape='\"',\n",
        "                           encoding = \"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGNQN0p1xvYp"
      },
      "source": [
        "## choosing chunk of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zY17Y2KDWhet"
      },
      "outputs": [],
      "source": [
        "size = 100000\n",
        "Job_df = df_Dataset.limit(size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3Fc066TLhRe"
      },
      "outputs": [],
      "source": [
        "type(Job_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCYqdniVjwGY"
      },
      "source": [
        "# Pre-processsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieKXULV3j8IO"
      },
      "source": [
        "### Exploratory analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaRQ2LhLj0ku"
      },
      "outputs": [],
      "source": [
        "Job_df.show(n = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUkJ2ZU9j0h7"
      },
      "outputs": [],
      "source": [
        "Job_df = Job_df.select(\"job_summary\")\n",
        "Job_df.show(n = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ukJ2kyOxvYr"
      },
      "source": [
        "#### Giving Id for each row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeH8BXMtj0gE"
      },
      "outputs": [],
      "source": [
        "indexed_rdd = Job_df.rdd.zipWithIndex()\n",
        "Job_df = indexed_rdd.map(lambda x: (x[1], x[0][0])).toDF([\"Id\", \"job_summary\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkVmLuZ7mh5V"
      },
      "outputs": [],
      "source": [
        "Job_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGL7HMGXj0Fc"
      },
      "outputs": [],
      "source": [
        "# checking missing values in the columns\n",
        "Job_df.select([count(when(isnan(c), c)).alias(c) for c in Job_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zUn4bFXj0DE"
      },
      "outputs": [],
      "source": [
        "#count distinct values in each column\n",
        "Job_df.select([countDistinct(c).alias(c) for c in Job_df.columns]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42-dyKZTllUH"
      },
      "source": [
        "### Duplicates check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoQD6ewcjz_8"
      },
      "outputs": [],
      "source": [
        "# show duplicates in Body column\n",
        "Job_df.groupBy(\"job_summary\").count().filter(\"count > 1\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVe3kM9GlpDe"
      },
      "outputs": [],
      "source": [
        "# Filter the rows where 'job_summary' starts with 'Job Title:\\nCerti'\n",
        "filtered_rows = Job_df.filter(col(\"job_summary\").startswith(\"Job Title:\\nCertified Nursing Assistant (CNA)\\nCompany\"))\n",
        "filtered_rows.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viD05Oxplows"
      },
      "outputs": [],
      "source": [
        "# ID number 1319\n",
        "row_with_id_1319 = Job_df.filter(Job_df['ID'] == 1319).collect()\n",
        "\n",
        "txt = row_with_id_1319[0][1]\n",
        "txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J8yiQGgmOng"
      },
      "outputs": [],
      "source": [
        "# ID number 1586\n",
        "row_with_id_1586 = Job_df.filter(Job_df['ID'] == 1586).collect()\n",
        "\n",
        "txt2 = row_with_id_1586[0][1]\n",
        "txt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nwRzE6SmSgT"
      },
      "outputs": [],
      "source": [
        "# Checking Equality of texts\n",
        "if txt == txt2:\n",
        "    print('Equal')\n",
        "else:\n",
        "    print('Not Equal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm1VjeEJmmhM"
      },
      "source": [
        "### Delete Duplicates Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di1Qu4rlmk2D"
      },
      "outputs": [],
      "source": [
        "Job_df = Job_df.dropDuplicates(['job_summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvYxt-5tm_s7"
      },
      "outputs": [],
      "source": [
        "Job_df.select([countDistinct(c).alias(c) for c in Job_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yyi-AEqGm_ql"
      },
      "outputs": [],
      "source": [
        "# checking again duplicates\n",
        "Job_df.groupBy(\"job_summary\").count().filter(\"count > 1\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyUy4TN8m_ny"
      },
      "outputs": [],
      "source": [
        "row_with_id_1586 = Job_df.filter(Job_df['ID'] == 1586).collect()\n",
        "row_with_id_1586"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI6_yAS0npTK"
      },
      "source": [
        "# Text cleaning and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PbihzgXm_lg"
      },
      "outputs": [],
      "source": [
        "Job_df = Job_df.select('Id',\"job_summary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaDAbp-bn5-r"
      },
      "source": [
        "### LoweCasing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tErqjlDdm_is"
      },
      "outputs": [],
      "source": [
        "Job_df = Job_df.withColumn('job_summary', lower(Job_df['job_summary']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZRXRdjQohHL"
      },
      "source": [
        "### Remove HTML Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJdudCgNm_f6"
      },
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "    pattern = re.compile('<.*?>')\n",
        "    return pattern.sub(r'', text) if text else text\n",
        "\n",
        "remove_html_tags_udf = udf(remove_html_tags, StringType())\n",
        "Job_df = Job_df.withColumn('job_summary', remove_html_tags_udf(Job_df['job_summary']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3FOznpvpVuM"
      },
      "source": [
        "###  Remove URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qYOtisopNRY"
      },
      "outputs": [],
      "source": [
        "def remove_url(text):\n",
        "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return pattern.sub(r'', text)\n",
        "\n",
        "remove_url_udf = udf(remove_url, StringType())\n",
        "Job_df = Job_df.withColumn('job_summary', remove_url_udf(Job_df['job_summary']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz2uESZgph30"
      },
      "source": [
        "### Remove Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUN3D1dMpNMI"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "remove_punctuation_udf = udf(remove_punctuation, StringType())\n",
        "Job_df = Job_df.withColumn('job_summary', remove_punctuation_udf(Job_df['job_summary']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjPqJsmxGjK2"
      },
      "source": [
        "### Remove numbers\n",
        "\n",
        "The following document has aroud 42 different number inside it, so we need to delet them.\n",
        "3x12 , 180000060000, 12003, 0, 4, 02142024, 05152024, 13, 556166975, 56166975 , 12 , 7 , 7, 100 , 133, 3467, 68100, 10 , 25, 50 , 100,\n",
        "100 , 20 , 3, 2, , 1, 0, 100, 15, 15, 15, 91, 401,36, 50, 2023, 2022, 2021 ,2020, 2019."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtLe-jovpNJl"
      },
      "outputs": [],
      "source": [
        "row_with_id_160 = Job_df.filter(Job_df['ID'] == 160).collect()\n",
        "row_with_id_160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0KRZ9xIw2r0"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(text):\n",
        "    pattern = re.compile(r'\\d+')\n",
        "    return pattern.sub(r'', text)\n",
        "\n",
        "remove_numbers_udf = udf(remove_numbers, StringType())\n",
        "Job_df = Job_df.withColumn('job_summary', remove_numbers_udf(Job_df['job_summary']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60FiTgIHpNAT"
      },
      "outputs": [],
      "source": [
        "row_with_id_160 = Job_df.filter(Job_df['ID'] == 160).collect()\n",
        "row_with_id_160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1opJKf--H4ao"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAIjcHRBJPRs"
      },
      "source": [
        "### Remove Non-ASCII characters:\n",
        "Some texts have some non-ASCII characters like (ã°â\\x9fâ\\x9fâ¡), so we need to delete them from texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzFylz4ZH4VR"
      },
      "outputs": [],
      "source": [
        "row_with_id_915 = Job_df.filter(Job_df['ID'] == 915).collect()\n",
        "row_with_id_915"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoowVAkiGogf"
      },
      "outputs": [],
      "source": [
        "def remove_non_ascii(text):\n",
        "    if text is None:\n",
        "        return None\n",
        "    return re.sub(r'[^\\x00-\\x7F]+', '', str(text))\n",
        "\n",
        "remove_non_ascii_udf = udf(remove_non_ascii, StringType())\n",
        "Job_df = Job_df.withColumn('job_summary', remove_non_ascii_udf(Job_df['job_summary']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhO9wxvhH4Qv"
      },
      "outputs": [],
      "source": [
        "row_with_id_915 = Job_df.filter(Job_df['ID'] == 915).collect()\n",
        "row_with_id_915"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suADkpQqKsDz"
      },
      "source": [
        "### Remove extra space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbbv0O2kKgkp"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import regexp_replace, col, trim\n",
        "\n",
        "def remove_extra_spaces(df, column_name):\n",
        "    df = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n",
        "    return df.withColumn(column_name, trim(col(column_name)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8Dbz0BIKgiI"
      },
      "outputs": [],
      "source": [
        "Job_df = remove_extra_spaces(Job_df, \"job_summary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETJ4cz1LCOg"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9trSn1OKggA"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer().setInputCol(\"job_summary\").setOutputCol(\"Tokens\")\n",
        "Job_df = tokenizer.transform(Job_df)\n",
        "Job_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvws70vULf5W"
      },
      "source": [
        "### Removing Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odmYIctMKgdf"
      },
      "outputs": [],
      "source": [
        "remove_stopwords = StopWordsRemover()\n",
        "stopwords = remove_stopwords.getStopWords()\n",
        "print(stopwords[:10])\n",
        "print(len(stopwords))\n",
        "\n",
        "remove_stopwords.setInputCol(\"Tokens\").setOutputCol(\"Tokens stopwords removed\")\n",
        "Job_df = remove_stopwords.transform(Job_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhDrnVUfLuB1"
      },
      "outputs": [],
      "source": [
        "# counting the number of tokens after stopwords removed\n",
        "Job_df = Job_df.withColumn(\"Number of tokens\", size(col(\"Tokens\")))\n",
        "Job_df = Job_df.withColumn(\"Number of tokens After stopwords removed\", size(col(\"Tokens stopwords removed\")))\n",
        "Job_df = Job_df.withColumn(\"Number of stopwords removed\", size(col(\"Tokens stopwords removed\")) - size(col(\"Tokens\")) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzlMusQnrPQB"
      },
      "outputs": [],
      "source": [
        "Job_df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbgLohL-91x2"
      },
      "source": [
        "###  Join the words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYGnoUbE9xAT"
      },
      "source": [
        "To join the words back together after tokenization and stopword removal, you can use the concat_ws function provided by PySpark. Here’s how you can do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSrn4KhK9nHM"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "Job_df = Job_df.withColumn(\"Cleaned_text\", concat_ws(\" \", col(\"Tokens stopwords removed\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7xpWGmYr28z"
      },
      "outputs": [],
      "source": [
        "Job_df.select(\"Cleaned_text\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXPxWKx9L5a5"
      },
      "source": [
        "## Final dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLsQ5XcBLu8K"
      },
      "outputs": [],
      "source": [
        "Job_df_proces = Job_df.select('Id', \"Cleaned_text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7xSwSLduTZC"
      },
      "outputs": [],
      "source": [
        "Job_df_proces.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE_L9bBXxvY-"
      },
      "source": [
        "### Creating Shingles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjKhcqCtQP6u"
      },
      "outputs": [],
      "source": [
        "def shingle(text, k):\n",
        "    shingles = set()\n",
        "    words = text.split()\n",
        "    for i in range(len(words) - k + 1):\n",
        "        shingles.add(' '.join(words[i:i+k]))\n",
        "    return list(shingles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVIt5xhg-s_Z"
      },
      "outputs": [],
      "source": [
        "k = 2\n",
        "shingle_udf = udf(lambda text: shingle(text, k), ArrayType(StringType()))\n",
        "Job_df_proces = Job_df_proces.withColumn(\"shingles\", shingle_udf(col(\"Cleaned_text\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-GefdX8kFhK"
      },
      "outputs": [],
      "source": [
        "Job_df_proces.select(\"shingles\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYy_lJU1xvY_"
      },
      "source": [
        "#### convert shingles to sparse vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98WSxCC7YYYy"
      },
      "outputs": [],
      "source": [
        "# Flatten the shingles column to get all unique shingles\n",
        "unique_shingles = Job_df_proces.select(explode(\"shingles\").alias(\"shingle\")).distinct().collect()\n",
        "shingle_index = {row[\"shingle\"]: idx for idx, row in enumerate(unique_shingles)}\n",
        "\n",
        "print(\"Unique shingles and their indices:\")\n",
        "print(shingle_index)\n",
        "\n",
        "def shingles_to_sparse_vector(shingles):\n",
        "    indices = sorted([shingle_index[sh] for sh in shingles if sh in shingle_index])\n",
        "    values = [1.0] * len(indices)\n",
        "    return Vectors.sparse(len(unique_shingles), indices, values)\n",
        "\n",
        "\n",
        "sparse_vector_udf = udf(lambda shingles: shingles_to_sparse_vector(shingles), VectorUDT())\n",
        "\n",
        "Job_df_proces = Job_df_proces.withColumn(\"features\", sparse_vector_udf(col(\"shingles\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aTBcxCRtx2l"
      },
      "outputs": [],
      "source": [
        "Job_df_proces.select(\"features\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3g3O904xvZA"
      },
      "source": [
        "## Implementing MinHashLSH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks6uAGuN5hcb"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "\n",
        "# Initialize MinHashLSH\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", seed=12345, numHashTables=20)\n",
        "model = mh.fit(Job_df_proces)\n",
        "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
        "hash = model.transform(Job_df_proces)\n",
        "\n",
        "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
        "# similarity join to Calculate Jaccard Distances.\n",
        "result = model.approxSimilarityJoin(hash, hash, 0.6, distCol=\"JaccardDistance\").select(\n",
        "    col(\"datasetA.id\").alias(\"idA\"),\n",
        "    col(\"datasetB.id\").alias(\"idB\"),\n",
        "    col(\"JaccardDistance\")\n",
        ")\n",
        "\n",
        "# Filter out self-pairs and display the results\n",
        "result_filtered = result.filter(col(\"idA\") < col(\"idB\"))\n",
        "\n",
        "end = time.time()\n",
        "computation_time = round(end - start, 3)\n",
        "print(\"Computation time: {} seconds\".format(computation_time))\n",
        "\n",
        "print('------------------------------------')\n",
        "\n",
        "\n",
        "# https://spark.apache.org/docs/2.2.0/ml-features.html\n",
        "# https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.MinHashLSH.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JLdmJFmc5Zm"
      },
      "outputs": [],
      "source": [
        "type(result_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a36H2GOvUN4"
      },
      "outputs": [],
      "source": [
        "result_filtered.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYWHOzqMv16I"
      },
      "outputs": [],
      "source": [
        "result_filtered.sort(result_filtered.JaccardDistance.asc()).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsuKatxZv12T"
      },
      "outputs": [],
      "source": [
        "result_filtered.sort(result_filtered.JaccardDistance.desc()).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JaccardDistance between 0.2 and 0.3\n",
        "filtered_result = result_filtered.filter((result_filtered.JaccardDistance >= 0.2) & (result_filtered.JaccardDistance <= 0.3))\n",
        "filtered_result.sort(filtered_result.JaccardDistance.asc()).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qstGEpIdSZbc"
      },
      "source": [
        "#### Result of minhash function (hash values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD4bpJ27Lus-"
      },
      "outputs": [],
      "source": [
        "hash.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YFR2Dy9WhfA"
      },
      "source": [
        "#### Sparce vector for first document , id = 160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEYHI8RjROKN"
      },
      "outputs": [],
      "source": [
        "hash.first()['features']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWOyzTwkWhfA"
      },
      "source": [
        "#### Signature vector for first document, id = 160\n",
        "Values inside DenseVector shows value of each hash function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9Z45IyVROG4"
      },
      "outputs": [],
      "source": [
        "hash.first()['hashes']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71mSHKCVzfu6"
      },
      "source": [
        "### Creating New dataframe in order to compare pair document with each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Job_df2 = Job_df.select('Id', \"Tokens stopwords removed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5TnIF1WhZ44"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def analyze_text_by_id(df: DataFrame, id_number: int):\n",
        "    row_with_id = df.filter(df['ID'] == id_number).collect()\n",
        "    print(row_with_id)\n",
        "\n",
        "    if not row_with_id:\n",
        "        print(f\"No row found with ID {id_number}\")\n",
        "        return\n",
        "\n",
        "    txt = row_with_id[0][1:][0]\n",
        "\n",
        "    print(f\"Type of txt: {type(txt)}\")\n",
        "    print(f\"Length of txt: {len(txt)}\")\n",
        "    print(f\"Fourth character in txt: {txt[3]}\")\n",
        "\n",
        "    return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rya7IKnL6GLR"
      },
      "outputs": [],
      "source": [
        "def analyze_lists(text1, text2):\n",
        "    list1 = [word for word in text1 if word]\n",
        "    list2 = [word for word in text2 if word]\n",
        "\n",
        "    num_words_list1 = len(list1)\n",
        "    num_words_list2 = len(list2)\n",
        "    num_unique_words_list1 = len(set(list1))\n",
        "    num_unique_words_list2 = len(set(list2))\n",
        "\n",
        "    common_words = set(list1).intersection(list2)\n",
        "    num_common_words = len(common_words)\n",
        "\n",
        "    percentage_common_list1 = (num_common_words / num_unique_words_list1) * 100 if num_unique_words_list1 > 0 else 0\n",
        "    percentage_common_list2 = (num_common_words / num_unique_words_list2) * 100 if num_unique_words_list2 > 0 else 0\n",
        "\n",
        "    return (num_words_list1, num_words_list2, num_unique_words_list1,\n",
        "            num_unique_words_list2, num_common_words,\n",
        "            percentage_common_list1, percentage_common_list2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpfXgancVA0h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_lwJOnlzKxD"
      },
      "source": [
        "### Comparing the Documents with 'ID' number of 1909 & 3014, which have Jaccard distance equal to 0.20\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u49-zW9RN8b"
      },
      "outputs": [],
      "source": [
        "txt1 = analyze_text_by_id(Job_df2, 1909)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPCo62tESFsv"
      },
      "outputs": [],
      "source": [
        "txt2 = analyze_text_by_id(Job_df2, 3014)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AymhKdfOSJz7"
      },
      "outputs": [],
      "source": [
        "# Comparing number of common words inside Documents with 'ID' number of 1909 & 3014\n",
        "\n",
        "(num_words_list1, num_words_list2, num_unique_words_list1,\n",
        " num_unique_words_list2, num_common_words,\n",
        " percentage_common_list1, percentage_common_list2) = analyze_lists(txt1, txt2)\n",
        "\n",
        "print(f\"Number of words in Text_1: {num_words_list1}\")\n",
        "print(f\"Number of words in Text_2: {num_words_list2}\")\n",
        "print(f\"Number of Unique words in Text_1: {num_unique_words_list1}\")\n",
        "print(f\"Number of Unique words in Text_2: {num_unique_words_list2}\")\n",
        "print(f\"Number of common Uniqe words: {num_common_words}\")\n",
        "print(f\"Percentage of common words in Text_1: {percentage_common_list1:.2f}%\")\n",
        "print(f\"Percentage of common words in Text_2: {percentage_common_list2:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07IsXkar8NtM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWCJl4vX8azC"
      },
      "source": [
        "### Comparing the Documents with 'ID' number of 3284 & 4955, which have Jaccard distance equal to 0.59"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF9mSBuq8NZz"
      },
      "outputs": [],
      "source": [
        "txt5 = analyze_text_by_id(Job_df2, 3284)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfLHNPgd8NW6"
      },
      "outputs": [],
      "source": [
        "txt6 = analyze_text_by_id(Job_df2, 4955)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THI9u5OA8zKZ"
      },
      "outputs": [],
      "source": [
        "# Comparing number of common words inside Documents with 'ID' number of 3284 & 4955\n",
        "\n",
        "(num_words_list1, num_words_list2, num_unique_words_list1,\n",
        " num_unique_words_list2, num_common_words,\n",
        " percentage_common_list1, percentage_common_list2) = analyze_lists(txt5, txt6)\n",
        "\n",
        "print(f\"Number of words in Text_1: {num_words_list1}\")\n",
        "print(f\"Number of words in Text_2: {num_words_list2}\")\n",
        "print(f\"Number of Unique words in Text_1: {num_unique_words_list1}\")\n",
        "print(f\"Number of Unique words in Text_2: {num_unique_words_list2}\")\n",
        "print(f\"Number of common Uniqe words: {num_common_words}\")\n",
        "print(f\"Percentage of common words in Text_1: {percentage_common_list1:.2f}%\")\n",
        "print(f\"Percentage of common words in Text_2: {percentage_common_list2:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFy2-wHaXnsh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzWOQBVSXzcu"
      },
      "source": [
        "### Comparing the Documents with 'ID' number of 503 & 948, which have Jaccard distance equal to 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuYF5xeJXnpn"
      },
      "outputs": [],
      "source": [
        "txt3 = analyze_text_by_id(Job_df2, 503)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8PuUgYzXnmK"
      },
      "outputs": [],
      "source": [
        "txt4 = analyze_text_by_id(Job_df2, 948)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMilCjeNX6Ax"
      },
      "outputs": [],
      "source": [
        "# Comparing number of common words inside Documents with 'ID' number of 503 & 948\n",
        "\n",
        "(num_words_list1, num_words_list2, num_unique_words_list1,\n",
        " num_unique_words_list2, num_common_words,\n",
        " percentage_common_list1, percentage_common_list2) = analyze_lists(txt3, txt4)\n",
        "\n",
        "print(f\"Number of words in Text_1: {num_words_list1}\")\n",
        "print(f\"Number of words in Text_2: {num_words_list2}\")\n",
        "print(f\"Number of Unique words in Text_1: {num_unique_words_list1}\")\n",
        "print(f\"Number of Unique words in Text_2: {num_unique_words_list2}\")\n",
        "print(f\"Number of common Uniqe words: {num_common_words}\")\n",
        "print(f\"Percentage of common words in Text_1: {percentage_common_list1:.2f}%\")\n",
        "print(f\"Percentage of common words in Text_2: {percentage_common_list2:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCUJpwLndWX7"
      },
      "source": [
        "## Cheking the Equality of documents:\n",
        "#### Cheking the Equality of documents with ['ID'] number 503 & 948, which they have Jaccard Distance equal to Zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQGDOn0bbt-y"
      },
      "outputs": [],
      "source": [
        "# ID number 503\n",
        "row_with_id_503 = Job_df.filter(Job_df['ID'] == 503).collect()\n",
        "txt = row_with_id_503[0][1:][0]\n",
        "txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QwJ2wR4bt1m"
      },
      "outputs": [],
      "source": [
        "# ID number 948\n",
        "row_with_id_948 = Job_df.filter(Job_df['ID'] == 948).collect()\n",
        "txt2 = row_with_id_948[0][1:][0]\n",
        "txt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCGTynSJbty7"
      },
      "outputs": [],
      "source": [
        "if txt == txt2:\n",
        "    print('Equal')\n",
        "else:\n",
        "    print('Not Equal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dence vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shingles_to_one_hot_vector(shingles):\n",
        "    vector = np.zeros(len(unique_shingles))\n",
        "    for sh in shingles:\n",
        "        if sh in shingle_index:\n",
        "            vector[shingle_index[sh]] = 1.0\n",
        "    return Vectors.dense(vector.tolist())\n",
        "\n",
        "# UDF to convert shingles to one-hot vectors\n",
        "one_hot_vector_udf = udf(lambda shingles: shingles_to_one_hot_vector(shingles), VectorUDT())\n",
        "\n",
        "Job_df_proces2 = Job_df_proces.withColumn(\"features_dence\", one_hot_vector_udf(col(\"shingles\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Job_df_proces2.select(\"features_dence\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Job_df_proces2.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "\n",
        "# Initialize MinHashLSH\n",
        "mh = MinHashLSH(inputCol=\"features_dence\", outputCol=\"hashes\", seed=12345, numHashTables=20)\n",
        "model = mh.fit(Job_df_proces2)\n",
        "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
        "hash = model.transform(Job_df_proces2)\n",
        "\n",
        "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
        "# similarity join to Calculate Jaccard Distances.\n",
        "result = model.approxSimilarityJoin(hash, hash, 0.6, distCol=\"JaccardDistance\").select(\n",
        "    col(\"datasetA.id\").alias(\"idA\"),\n",
        "    col(\"datasetB.id\").alias(\"idB\"),\n",
        "    col(\"JaccardDistance\")\n",
        ")\n",
        "\n",
        "# Filter out self-pairs and display the results\n",
        "result_filtered = result.filter(col(\"idA\") < col(\"idB\"))\n",
        "\n",
        "end = time.time()\n",
        "computation_time = round(end - start, 3)\n",
        "print(\"Computation time: {} seconds\".format(computation_time))\n",
        "\n",
        "print('------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
